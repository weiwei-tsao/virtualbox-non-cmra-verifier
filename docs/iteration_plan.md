ğŸ—ï¸ å½“å‰æ¶æ„çŠ¶æ€
åŠŸèƒ½ å½“å‰çŠ¶æ€ èƒ½å¦å®ç°ä½ çš„éœ€æ±‚

1. å¢é‡çˆ¬å– âŒ éœ€è¦æ‰‹åŠ¨ä¼ å…¥æ‰€æœ‰ links ä¸èƒ½ - ç¼ºå°‘å‘ç°æœºåˆ¶
2. å•ç‹¬è§£æ âŒ æ²¡æœ‰ä¿å­˜åŸå§‹ HTML ä¸èƒ½ - æ— æ•°æ®æº
3. æ–­ç‚¹ç»­ä¼  âš ï¸ æœ‰å¢é‡å†™å…¥ï¼Œä½†æ— ç‰ˆæœ¬æ§åˆ¶ éƒ¨åˆ† - æ— æ³•è¯†åˆ«"æ–°æ•°æ®"
   ğŸ“‹ è¯¦ç»†åˆ†æä¸è§£å†³æ–¹æ¡ˆ
   1ï¸âƒ£ å¢é‡çˆ¬å– - åŠæ—¶æ›´æ–°ä¸Šæ¶/ä¸‹æ¶
   å½“å‰é—®é¢˜ï¼š
   æŸ¥çœ‹ scraper.go:53-135 - æ¥æ”¶å›ºå®šçš„ links æ•°ç»„
   æ²¡æœ‰è‡ªåŠ¨å‘ç°æœºåˆ¶ - æ— æ³•çŸ¥é“ AnytimeMailbox ç½‘ç«™æ–°å¢äº†å“ªäº›åº—
   æ¯æ¬¡éƒ½è¦çˆ¬å…¨éƒ¨é“¾æ¥
   å®ç°æ–¹æ¡ˆï¼š
   // æ–°å¢åŠŸèƒ½æ¨¡å—ï¼šDiscoveryï¼ˆå‘ç°ï¼‰

// Step 1: çˆ¬å–åˆ—è¡¨é¡µï¼Œè·å–æ‰€æœ‰ mailbox é“¾æ¥
func DiscoverAllLinks(ctx context.Context) ([]string, error) {
// çˆ¬å– https://www.anytimemailbox.com/l (åˆ—è¡¨é¡µ)
// æå–æ‰€æœ‰ /s/xxx é“¾æ¥
}

// Step 2: å¯¹æ¯”å·®å¼‚
func FindChanges(discovered []string, existing map[string]Mailbox) Changes {
return Changes{
New: discovered - existing.links, // æ–°ä¸Šæ¶
Removed: existing.links - discovered, // ä¸‹æ¶
}
}

// Step 3: å¢é‡å¤„ç†
func IncrementalCrawl(ctx context.Context) {
discovered := DiscoverAllLinks(ctx)
existing := store.FetchAllMap(ctx)
changes := FindChanges(discovered, existing)

    // åªçˆ¬æ–°å¢çš„
    ScrapeAndUpsert(ctx, changes.New)

    // æ ‡è®°ä¸‹æ¶ï¼ˆè®¾ç½® Active=falseï¼‰
    store.MarkInactive(ctx, changes.Removed)

}
æ–°å¢ APIï¼š
POST /api/crawl/discover - åªå‘ç°å·®å¼‚ï¼Œä¸çˆ¬å–
POST /api/crawl/incremental - è‡ªåŠ¨å‘ç° + å¢é‡çˆ¬å–
2ï¸âƒ£ å•ç‹¬è§£æ - æ— éœ€é‡æ–°çˆ¬å–
å½“å‰é—®é¢˜ï¼š
model.go:19-33 - æ²¡æœ‰ RawHTML å­—æ®µ
æ¯æ¬¡è°ƒæ•´è§£æå™¨éƒ½è¦é‡æ–°è¯·æ±‚ AnytimeMailbox
å®ç°æ–¹æ¡ˆï¼š
// Step 1: æ‰©å±•æ•°æ®æ¨¡å‹
type Mailbox struct {
// ... ç°æœ‰å­—æ®µ ...

    // æ–°å¢å­—æ®µ
    RawHTML        string    `json:"-" firestore:"rawHTML,omitempty"`  // åŸå§‹HTMLï¼ˆä¸è¿”å›ç»™å‰ç«¯ï¼‰
    ParserVersion  string    `firestore:"parserVersion,omitempty"`     // è§£æå™¨ç‰ˆæœ¬ï¼ˆå¦‚ "v1.0"ï¼‰
    LastParsedAt   time.Time `firestore:"lastParsedAt,omitempty"`      // æœ€åè§£ææ—¶é—´

}

// Step 2: çˆ¬å–æ—¶ä¿å­˜ HTML
func ScrapeAndUpsert(...) {
body, _ := fetcher.Fetch(ctx, link)
html, _ := io.ReadAll(body) // è¯»å– HTML

    parsed, _ := ParseMailboxHTML(bytes.NewReader(html), link)
    parsed.RawHTML = string(html)           // âœ… ä¿å­˜åŸå§‹HTML
    parsed.ParserVersion = "v1.0"           // âœ… æ ‡è®°ç‰ˆæœ¬
    parsed.LastParsedAt = time.Now()

}

// Step 3: å•ç‹¬é‡æ–°è§£æ
func ReprocessFromDB(ctx context.Context, targetVersion string) {
mailboxes, \_ := store.FetchAll(ctx)

    for _, mb := range mailboxes {
        // åªå¤„ç†æœ‰HTMLä¸”ç‰ˆæœ¬ä¸åŒ¹é…çš„
        if mb.RawHTML != "" && mb.ParserVersion != targetVersion {
            // é‡æ–°è§£æ
            reparsed, _ := ParseMailboxHTML(
                strings.NewReader(mb.RawHTML),
                mb.Link,
            )

            // ä¿ç•™ IDã€RawHTMLã€CrawlRunID
            reparsed.ID = mb.ID
            reparsed.RawHTML = mb.RawHTML
            reparsed.ParserVersion = targetVersion  // âœ… æ›´æ–°ç‰ˆæœ¬
            reparsed.LastParsedAt = time.Now()

            store.Update(ctx, reparsed)
        }
    }

}
æ–°å¢ APIï¼š
POST /api/crawl/reprocess - ä»æ•°æ®åº“é‡æ–°è§£æ
{
"targetVersion": "v1.1",
"onlyOutdated": true // åªå¤„ç†æ—§ç‰ˆæœ¬
}
3ï¸âƒ£ æ–­ç‚¹ç»­ä¼  + åªè§£ææ–°æ•°æ®
å½“å‰é—®é¢˜ï¼š
å·²æœ‰å¢é‡å†™å…¥ï¼ˆæ¯ 100 æ¡ï¼‰ï¼Œä½†æ— æ³•è¯†åˆ«"å“ªäº›æ˜¯æ–°æ•°æ®"
é‡æ–°è§£æä¼šå¤„ç†æ‰€æœ‰è®°å½•
å®ç°æ–¹æ¡ˆï¼š
// æ–¹æ¡ˆ A: åŸºäºç‰ˆæœ¬å·ï¼ˆæ¨èï¼‰
func ReprocessFromDB(ctx context.Context, opts ReprocessOptions) {
query := store.Query()

    if opts.OnlyOutdated {
        // âœ… åªå¤„ç†ç‰ˆæœ¬ä¸åŒ¹é…çš„
        query = query.Where("parserVersion", "!=", opts.TargetVersion)
    }

    if opts.SinceTime != nil {
        // âœ… åªå¤„ç†ç‰¹å®šæ—¶é—´åæ›´æ–°çš„
        query = query.Where("lastValidatedAt", ">", opts.SinceTime)
    }

    // å¢é‡å¤„ç† + æ–­ç‚¹ç»­ä¼ 
    const batchSize = 100
    for {
        batch := query.Limit(batchSize).Offset(processed)

        for _, mb := range batch {
            reparsed := ParseMailboxHTML(...)
            store.Update(ctx, reparsed)
            processed++

            // æ¯100æ¡å†™å…¥è¿›åº¦
            if processed % 100 == 0 {
                SaveProgress(processed)  // âœ… æ–­ç‚¹ç»­ä¼ 
            }
        }

        if len(batch) < batchSize {
            break  // å¤„ç†å®Œæ¯•
        }
    }

}
æ–­ç‚¹ç»­ä¼ æœºåˆ¶ï¼š
// åœ¨ CrawlRun ä¸­è®°å½•è¿›åº¦
type CrawlRun struct {
// ... ç°æœ‰å­—æ®µ ...
ProcessedCount int `firestore:"processedCount,omitempty"`
LastProcessedID string `firestore:"lastProcessedID,omitempty"`
}

// æ¢å¤æ—¶ä»æ–­ç‚¹ç»§ç»­
func ResumeReprocess(runID string) {
run := store.GetCrawlRun(runID)
startFrom := run.LastProcessedID // âœ… ä»ä¸Šæ¬¡ä¸­æ–­çš„åœ°æ–¹ç»§ç»­
}
ğŸ¯ æ¨èå®ç°ä¼˜å…ˆçº§
æˆ‘å»ºè®®æŒ‰è¿™ä¸ªé¡ºåºå®ç°ï¼š
é˜¶æ®µ 1ï¼šå•ç‹¬è§£æï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰â­â­â­
åŸå› ï¼šç«‹å³è§£å†³å½“å‰ç—›ç‚¹ - ä¸éœ€è¦é‡æ–°çˆ¬ 2073 æ¡æ•°æ®
âœ… æ·»åŠ  RawHTML, ParserVersion, LastParsedAt å­—æ®µ
âœ… ä¿®æ”¹ scraper.go ä¿å­˜ HTML
âœ… åˆ›å»º ReprocessFromDB å‡½æ•°
âœ… æ·»åŠ  POST /api/crawl/reprocess API
âš ï¸ é‡è¦ï¼šé‡æ–°çˆ¬å–ä¸€æ¬¡ä¿å­˜ HTMLï¼ˆä¹‹åå°±ä¸éœ€è¦äº†ï¼‰
æ”¶ç›Šï¼šè§£æå™¨è°ƒè¯•æ—¶é—´ä» 30 åˆ†é’Ÿï¼ˆé‡æ–°çˆ¬å–ï¼‰â†’ 2 åˆ†é’Ÿï¼ˆé‡æ–°è§£æï¼‰
é˜¶æ®µ 2ï¼šæ–­ç‚¹ç»­ä¼  + å¢é‡è§£æ â­â­
åŸå› ï¼šæå‡å¯é æ€§å’Œæ•ˆç‡
âœ… æ·»åŠ  ProcessedCount, LastProcessedID åˆ° CrawlRun
âœ… å®ç°ç‰ˆæœ¬è¿‡æ»¤é€»è¾‘
âœ… å®ç°æ¢å¤æœºåˆ¶
æ”¶ç›Šï¼šè§£æä¸­æ–­åå¯ç»§ç»­ï¼Œé¿å…é‡å¤å¤„ç†
é˜¶æ®µ 3ï¼šå¢é‡çˆ¬å–ï¼ˆé•¿æœŸä¼˜åŒ–ï¼‰â­
åŸå› ï¼šç›®å‰ mailbox æ•°é‡ä¸å¤§ï¼Œå…¨é‡çˆ¬å–å°šå¯æ¥å—
å®ç° DiscoverAllLinksï¼ˆçˆ¬åˆ—è¡¨é¡µï¼‰
å®ç°å·®å¼‚å¯¹æ¯”é€»è¾‘
æ·»åŠ  POST /api/crawl/incremental API
æ·»åŠ å®šæ—¶ä»»åŠ¡ï¼ˆæ¯å¤©è‡ªåŠ¨å¢é‡çˆ¬å–ï¼‰
æ”¶ç›Šï¼šä»å…¨é‡ 2073 æ¡ â†’ å¢é‡ 10-50 æ¡/å¤©
